%%LaTeX Source for The Setup Guide for the CSH Beowulf Cluster

\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, urlcolor=magenta]{hyperref}

\title{CSH Compute Cluster Node Setup Guide}
\author{Zach Hart}

\begin{document}
\maketitle

\section{Things to know before you start}

So you want to add (or replace) a node to the existing cluster.
This should be fairly straightforward, as it's been done a bunch and we've encountered pretty much every possible roadblock that you might encounter.
That being said, if you followed this guide and something went wrong, re-read the guide and make sure you didn't miss something.
A more condensed version of this guide can be found at the bottom of the \href{https://wiki.csh.rit.edu/wiki/Beowulf_Cluster}{CSH Wiki Entry} on the cluster.
If you're still having troubles, check out the \href{https://slurm.schedmd.com/troubleshoot.html}{SchedMD Troubleshooting Guide}.

\subsection{Unusual Setup Constraints}

If the new node is in IH, OpComm should have already let you know of a few network environment constraints you'll need to deal with.
The most obvious of which is that you will only be able to access this node from within RIT's Network.
For a CSHer, this presents no real challenge - either use the CSH or RIT VPN, or simply use another machine as a bridge into the network (for example, \texttt{ssh} into \texttt{shell.csh.rit.edu} and then into the node you want to access). 

\section{Basic Setup Tasks}

\subsection{Node Provisioning}

Currently, our nodes have been provisioned for us by OpComm, and are all LinkedIn Nodes (2 sockets, 12 Cores per socket, 48 Gigabytes of RAM) running RHEL 7.
To keep things in the guide simple, we're going to assume the node you're trying to add is identical.
If your node is different, note that many things in this guide will need to be tailored.
At the minimum, your node should be running RHEL/CentOS, otherwise our setup scripts will fail.

\subsection{Initial Setup}

When you first get into a node, create the user accounts that you'll want and make sure those users are in the \texttt{wheel} group so that your user has access to \texttt{sudo}.
While it's not completely necessary to have a user account other than root (and the slurm and munge users that will be created for you), it's generally good practice to keep one around.
The next thing that should be done is to update all of the packages on the machine - \textbf{however}, you don't need to do this yourself. Running \texttt{wget http://sega.csh.rit.edu/compute-node.sh \&\& chmod 755 compute-node.sh} will download and set the permissions on the setup script that we've created, allowing you to execute the script.
The script does still need root access, so as root, run \texttt{./compute-node.sh} - the script doesn't take too long to run, so sit back and watch for any errors.
A quick breakdown of what the script does is as follows

\begin{itemize}[noitemsep]
\item Update system packages
\item Adds \texttt{epel-release} to the system
\item Installs some desired packages \texttt{git}, \texttt{vim}, \texttt{htop}, etc.
\item Grabs RIT's slurm repo and CSH's configuration files
\item Installs \& configures slurm and MUNGE
\item Starts and enables the slurm daemon
\end{itemize}

Keep in mind, that for a more seamless update of the cluster, you should update the \texttt{slurm.conf} with the system details of the new node \textit{before} distributing it to the new system.
To re-distribute this file to all existing nodes in the cluster, run \texttt{slurm-update.py} found in \texttt{/opt/slurm-update} on the master node.
\textit{Keep in mind} that this file will also need to be updated with the name of the new node - although it can be added \textit{after} you've run it if you've updated \texttt{slurm.conf} \textit{before} running \texttt{compute-node.sh} on the new node.

\subsection{Cleaning up}
If you've run \texttt{compute-node.sh} without any problems, and \texttt{slurmd} is up and running, you can go ahead and remove the \texttt{compute-node.sh} script from the machine.
All the files that were downloaded by this script (that weren't subsequently moved to their intended location) were placed in \texttt{/tmp}, so you don't need to worry about them.

\subsection{Networking}
You'll likely need to open a port in your firewall for \texttt{slurmd} to communicate with \texttt{slurmctld} on the master node.
To accomplish this, simply run \texttt{firewall-cmd -{}-permanent -{}-zone=public -{}-add-port=6818/tcp \&\& firewall-cmd -{}-reload} as root on the new compute node.

\subsection{Updating the Cluster}
At this point, your new node should be good to go, but the cluster doesn't really have any idea that it exists.
To "update" the cluster so-to-speak, on any node run \texttt{scontrol reconfigure} as root.
At this point, running \texttt{sinfo} from any node should give you a brief message about the state of the cluster.
Ideally, if no jobs are being run, your new node will be listed alongside the others as \texttt{idle}.
If your machine has any other state, or the state has an asterisk appended to it, there are some issues - check out the troubleshooting section to see if we know how to fix it.

\section{Troubleshooting}

This should probably be the first place you look if you're having issues setting up a node - issues with the cluster as a whole are better Googled, or solved by a look through the official troubleshooting guide linked at the top of this guide.
The issues here and their solutions are going to be generally specific to CSH's specific setup.

\subsection{Common Errors}

\subsubsection*{The \texttt{compute-node.sh} setup script fails}
There are a couple reasons this might happen, paying attention as the script runs to determine where things might fail is the best way to get a head start on diagnosing this issue.
If there are issues with \texttt{wget} grabbing files from the master node, make sure that \texttt{httpd} is running on \texttt{sega.csh.rit.edu}.
If there are issues downloading packages from \texttt{epel-release} (namely, \texttt{fail2ban}), ask OpComm to make sure that the proxy settings for yum were set up correctly. Remember, this script must be run as root in order to work.

\subsubsection*{\texttt{slurmd} is reporting issues with \texttt{munge} or \texttt{munged}}
Run \texttt{journalctl -xe} or check the slurm logs at \texttt{/var/log/slurmd.log}, if the issues appear to do with file ownership, read thoroughly through the \href{https://github.com/dun/munge/wiki/Installation-Guide}{MUNGE setup guide}.
We use the default file permissions, munge files should be owned by the munge user.
Once you've confirmed the file permissions, check to see if \texttt{munged} is running - If it is, kill the process.
Restart the daemon by running \texttt{sudo -u munge /usr/sbin/munged}, \texttt{munged} must be running as the munge user.
Restart the slurm daemon by running \texttt{systemctl restart slurmd} as root, and check the logs to see if things are okay now.

\subsubsection*{A new node can't communicate with the cluster}
First, try \texttt{ping}ing the master node - this will test if there are general networking problems between you and the rest of the cluster.
If you're able to send packets back and forth, the issue lies someplace else.
\textbf{Make sure that you've opened the correct port in your firewall.}
Next, run \texttt{ntpdate ntp.rit.edu} - all of the nodes should be synced to RIT's NTP server - keeping the clocks of the nodes as close together as possible is important.
If your machine jumps in either direction by more than a fraction of a second, run \texttt{scontrol reconfigure} followed by \texttt{sinfo} and see if you're able to communicate again.
Keep in mind that any state that ends in an asterisk (e.g. \texttt{idle*}, \texttt{down*}) means that the master \textbf{cannot} communicate properly with the node, and the state that it's assigned to the node is an assumption. 

It might be worth it to try and run the command \texttt{scontrol update nodename=<nodename> state=resume} to try and force the cluster to assign a new state to the node.
If the cluster is able to establish a connection, your machine will change state.
If the state is not \texttt{idle}, run \texttt{scontrol show node} on the node that isn't communicating to see what the status of the node is.
If the status changes to any assumed status, it's likely that whatever you've tried hasn't worked - you'll need to try something else.
Try running \texttt{scontrol ping} from the node that is having problems - If the node believes that the master is down, check \texttt{slurm.conf} to make sure that you have the right settings for the master node.
If it shows the master as UP, it's time to check the slurmd logs.
Logs can be found at \texttt{/var/log/slurmd.log}, running \texttt{journalctl -xe} and \texttt{systemctl status slurmd} may also prove helpful.



\end{document}